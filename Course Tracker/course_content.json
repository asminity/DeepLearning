{
  "sections": [
    {"title":"Introduction","lectures":["How to learn from this course","Using Udemy like a pro"]},
    {"title":"Download all course materials","lectures":["Downloading and using the code","My policy on code-sharing"]},
    {"title":"Concepts in deep learning","lectures":["What is an artificial neural network?","How models learn","The role of DL in science and knowledge","Running experiments to understand DL","Are artificial neurons like biological neurons?"]},
    {"title":"About the Python tutorial","lectures":["Should you watch the Python tutorial?"]},
    {"title":"Math, numpy, PyTorch","lectures":["Introduction to this section","Spectral theories in mathematics","Terms and datatypes in math and computers","Converting reality to numbers","Vector and matrix transpose","OMG it's the dot product!","Matrix multiplication","Softmax","Logarithms","Entropy and cross-entropy","Min/max and argmin/argmax","Mean and variance","Random sampling and sampling variability","Reproducible randomness via seeding","The t-test","Derivatives: intuition and polynomials","Derivatives find minima","Derivatives: product and chain rules"]},
    {"title":"Gradient descent","lectures":["Overview of gradient descent","What about local minima?","Gradient descent in 1D","CodeChallenge: unfortunate starting value","Gradient descent in 2D","CodeChallenge: 2D gradient ascent","Parametric experiments on g.d.","CodeChallenge: fixed vs. dynamic learning rate","Vanishing and exploding gradients","Tangent: Notebook revision history"]},
    {"title":"ANNs (Artificial Neural Networks)","lectures":["The perceptron and ANN architecture","A geometric view of ANNs","ANN math part 1 (forward prop)","ANN math part 2 (errors, loss, cost)","ANN math part 3 (backprop)","ANN for regression","CodeChallenge: manipulate regression slopes","ANN for classifying qwerties","Learning rates comparison","Multilayer ANN","Linear solutions to linear problems","Why multilayer linear models don't exist","Multi-output ANN (iris dataset)","CodeChallenge: more qwerties!","Comparing the number of hidden units","Depth vs. breadth: number of parameters","Defining models using sequential vs. class","Model depth vs. breadth","CodeChallenge: convert sequential to class","Diversity of ANN visual representations","Reflection: Are DL models understandable yet?"]},
    {"title":"Overfitting and cross-validation","lectures":["What is overfitting and is it as bad as they say?","Cross-validation","Generalization","Cross-validation -- manual separation","Cross-validation -- scikitlearn","Cross-validation -- DataLoader","Splitting data into train, devset, test","Cross-validation on regression"]},
    {"title":"Regularization","lectures":["Regularization: Concept and methods","train() and eval() modes","Dropout regularization","Dropout regularization in practice","Dropout example 2","Weight regularization (L1/L2): math","L2 regularization in practice","L1 regularization in practice","Training in mini-batches","Batch training in action","The importance of equal batch sizes","CodeChallenge: Effects of mini-batch size"]},
    {"title":"Metaparameters (activations, optimizers)","lectures":["What are 'metaparameters'?","The 'wine quality' dataset","CodeChallenge: Minibatch size in the wine dataset","Data normalization","The importance of data normalization","Batch normalization","Batch normalization in practice","CodeChallenge: Batch-normalize the qwerties","Activation functions","Activation functions in PyTorch","Activation functions comparison","CodeChallenge: Compare relu variants","CodeChallenge: Predict sugar","Loss functions","Loss functions in PyTorch","More practice with multioutput ANNs","Optimizers (minibatch, momentum)","SGD with momentum","Optimizers (RMSprop, Adam)","Optimizers comparison","CodeChallenge: Optimizers and... something","CodeChallenge: Adam with L2 regularization","Learning rate decay","How to pick the right metaparameters"]},
    {"title":"FFNs (Feed-Forward Networks)","lectures":["What are fully-connected and feedforward networks?","The MNIST dataset","FFN to classify digits","CodeChallenge: Binarized MNIST images","CodeChallenge: Data normalization","Distributions of weights pre- and post-learning","CodeChallenge: MNIST and breadth vs. depth","CodeChallenge: Optimizers and MNIST","Scrambled MNIST","Shifted MNIST","CodeChallenge: The mystery of the missing 7","Universal approximation theorem"]},
    {"title":"More on data","lectures":["Anatomy of a torch dataset and dataloader","Data size and network size","CodeChallenge: unbalanced data","What to do about unbalanced designs?","Data oversampling in MNIST","Data noise augmentation (with devset+test)","Data feature augmentation","Getting data into colab","Save and load trained models","Save the best-performing model","Where to find online datasets"]},
    {"title":"Measuring model performance","lectures":["Two perspectives of the world","Accuracy, precision, recall, F1","APRF in code","APRF example 1: wine quality","APRF example 2: MNIST","CodeChallenge: MNIST with unequal groups","Computation time","Better performance in test than train?"]},
    {"title":"FFN milestone projects","lectures":["Project 1: A gratuitously complex adding machine","Project 1: My solution","Project 2: Predicting heart disease","Project 2: My solution","Project 3: FFN for missing data interpolation","Project 3: My solution"]},
    {"title":"Weight inits and investigations","lectures":["Explanation of weight matrix sizes","A surprising demo of weight initializations","Theory: Why and how to initialize weights","CodeChallenge: Weight variance inits","Xavier and Kaiming initializations","CodeChallenge: Xavier vs. Kaiming","CodeChallenge: Identically random weights","Freezing weights during learning","Learning-related changes in weights","Use default inits or apply your own?"]},
    {"title":"Autoencoders","lectures":["What are autoencoders and what do they do?","Denoising MNIST","CodeChallenge: How many units?","AEs for occlusion","The latent code of MNIST","Autoencoder with tied weights"]},
    {"title":"Running models on a GPU","lectures":["What is a GPU and why use it?","Implementation","CodeChallenge: Run an experiment on the GPU"]},
    {"title":"Convolution and transformations","lectures":["Convolution: concepts","Feature maps and convolution kernels","Convolution in code","The Conv2 class in PyTorch","Convolution parameters (stride, padding)","Transpose convolution","Max/mean pooling","Pooling in PyTorch","To pool or to stride?","Image transforms","Creating and using custom DataLoaders"]},
    {"title":"Understand and design CNNs","lectures":["The canonical CNN architecture","CNN to classify MNIST digits","CNN on shifted MNIST","Classify Gaussian blurs","Examine feature map activations","CodeChallenge: Softcoded internal parameters","CodeChallenge: How wide the FC?","Do autoencoders clean Gaussians?","CodeChallenge: AEs and occluded Gaussians","CodeChallenge: Custom loss functions","Discover the Gaussian parameters","The EMNIST dataset (letter recognition)","Dropout in CNNs","CodeChallenge: How low can you go?","CodeChallenge: Varying number of channels","So many possibilities! How to create a CNN?"]},
    {"title":"CNN milestone projects","lectures":["Project 1: Import and classify CIFAR10","Project 1: My solution","Project 2: CIFAR-autoencoder","Project 3: FMNIST","Project 4: Psychometric functions in CNNs"]},
    {"title":"Transfer learning","lectures":["Transfer learning: What, why, and when?","Transfer learning: MNIST -> FMNIST","CodeChallenge: letters to numbers","Famous CNN architectures","Transfer learning with ResNet-18","CodeChallenge: VGG-16","Pretraining with autoencoders","CIFAR10 with autoencoder-pretrained model"]},
    {"title":"Style transfer","lectures":["What is style transfer and how does it work?","The Gram matrix (feature activation covariance)","The style transfer algorithm","Transferring the screaming bathtub","CodeChallenge: Style transfer with AlexNet"]},
    {"title":"Generative adversarial networks","lectures":["GAN: What, why, and how","Linear GAN with MNIST","CodeChallenge: Linear GAN with FMNIST","CNN GAN with Gaussians","CodeChallenge: Gaussians with fewer layers","CNN GAN with FMNIST","CodeChallenge: CNN GAN with CIFAR"]},
    {"title":"RNNs (Recurrent Neural Networks) and GRU/LSTM","lectures":["Leveraging sequences in deep learning","How RNNs work","The RNN class in PyTorch","Predicting alternating sequences","CodeChallenge: sine wave extrapolation","More on RNNs: Hidden states, embeddings","GRU and LSTM","The LSTM and GRU classes","Lorem ipsum"]},
    {"title":"Ethics of deep learning","lectures":["Will AI save us or destroy us?","Example case studies","Some other possible ethical scenarios","Will deep learning take our jobs?","Accountability and making ethical AI"]},
    {"title":"Where to go from here?","lectures":["How to learn topic X in deep learning?","How to read academic DL papers"]}
    ]
  }